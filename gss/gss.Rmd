---
title: "The General Social Survey, 2014"
subtitle: "An exploratory analysis"
author: "Derrick Lin"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library("tufte")
library("corrplot")
library("dplyr")
library("ggplot2")
library("psych")
library("softImpute")

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

# DataFrame -> CorrelationMatrix
scree = function(d_f) {
  cor_matrix = cor(d_f, use="pairwise.complete.obs")
  cor_matrix[is.na(cor_matrix)] = 0
  return(cor_matrix)
}

# CorrelationMatrix -> Nat -> FactorAnalysis
# side-effect: corrplot of scores
factor_analysis = function(d_f, num_components, covar=TRUE) {
  factors = fa(d_f, num_components, rotate="oblimin", fm="pa", covar=covar)
  corrplot(factors$score.cor)
  return(factors)
}

# Nat -> Loadings -> Keys -> Matrix of [Questions], [Codings]
getQuestions = function(n_qs=10, loadings, keys) {
  fact_questions = lapply(loadings, function(c) as.character(keys$values[order(abs(c), decreasing=TRUE)][1:n_qs]))
  fact_codings = lapply(loadings, function(c) as.character(keys$X[order(abs(c), decreasing=TRUE)][1:n_qs]))
  return(cbind(fact_questions, fact_codings))
}

# initial loading
df = read.csv("~/repos/signal/gss/gss.csv")
keys = read.csv("~/repos/signal/gss/keys.csv")
keys = keys[keys$X %in% names(df),]
keys = keys[order(keys$X),]
```

# The General Social Survey

The General Social Survey [run out of the National Opinion Research Center at the University of Chicago, and their longest running project] collects data on the demographics, opinions, and behaviors of American society. It is a tremendous/ambitious effort: both sampling and survey methodology are best described as "scrupulous", each respondent is interviewed in-person (rather than simply given a questionnaire to fill out), and every interview requires 90 minutes to administer. [The interview has also been fully translated to and, when appropriate, administered in Spanish.]

It is accordingly influential -- the most frequently analyzed source of information in the social sciences, save for the US Census. Over 1,000 scholarly publications using the data appear every year. Government, national, and international media frequently cite the GSS. Yet it has garnered little to no attention in the public consciousness. Few would recognize the name. Unobtrusive and unassuming.

# Given a dataset 2014

Presumably, respondent's answers should be explainable by a smaller number of underlying variables. An unsupervised technique that learns a *latent variable model* is appropriate. Selecting principal component analysis is not incorrect, but exploratory factor analysis is a finer choice. Humans are reasonably well understood already, and the technique allows for oblique axes/rotations.

The downside: the number of latent factors must be chosen up front. We can examine the scree plot[checking the eigenvalues of the correlation matrix]:

```{r warning = FALSE}
# Conduct a scree test:
cm = scree(df)
eig_cor_matrix = eigen(cm)
qplot(1:50, eig_cor_matrix$values[order(abs(eig_cor_matrix$values), decreasing=TRUE)][1:50], ylab = "Eigenvalue")
```

Look closely; the points level off at the 16th factor.

It looks like the elbow is at about.. 16, with most in the first 6. Let's go ahead an conduct a factor analysis. Then for each factor we'll find the questions most loaded for it.

# Factor Analysis

Factors account for variance. What is the main source of variance in the answers in Americans?

Given Trump, Brexit, and the general hyperpolarization of American politics, "politics" is a natural guess. 

```{r warning = FALSE, echo = FALSE}
scree_n = 16
factors = factor_analysis(cm, scree_n) # side-effect: corrplot(factors$score.cor)

# the rotation matrix; needed after imputation
loadings = as.data.frame(factors$loadings[,1:scree_n])

# Grab the related questions for the factors
qs = getQuestions(20, loadings, keys)
qs[,1][1:6]
```

Wow, freedom of speech is first? I wonder if that differs by subgroups -- maybe gender, race, political orientation, religion, religiosity, profession, background.

```{r echo = FALSE}

```

Use an advanced imputation method (read the paper)

```{r warning = FALSE}
# imputation
fit = softImpute(as.matrix(df), rank.max=16, lambda = 10, type="svd")
ndf = as.data.frame(complete(as.matrix(df), fit))

# re-compute correlation matrix, conduct factor analysis
cm = scree(ndf)
factors = factor_analysis(cm, scree_n) # side-effect: corrplot(factors$score.cor)

# the rotation matrix; needed after imputation
loadings = as.data.frame(factors$loadings[,1:scree_n])

# compute the scores (change of basis)
scores = as.matrix(ndf) %*% as.matrix(factors$loadings[,1:scree_n])

```

Investigate

```{r}
# qplot(scale(scores[,1])) # THAT'S NOT GOOD, breaks assumptions. Confounds.
# 
# freedom_questions = c("spkhomo", "libhomo", "librac", "spkrac", "colmslm", "libmslm", "spkmslm", "life", "sprtprsn", "colrac", "comprend", "colcom", "colhomo", "libcom", "spkcom", "libmil", "spkmil", "colmil", "trust") # definitely there are things unrelated to freedom of speech
# 
# for (q in freedom_questions) {
#   print(qplot(df[social_scientists, q], xlab=q))
# }
```

Not very good results, let's try just the civil liberties questions?

```{r}
# must preserve the ordering in the keys!
civil_liberties_qs = c("colath", "colcom", "colhomo", "colmil", "colmslm", "colrac",
                       "libath", "libcom", "libhomo", "libmil", "libmslm", "librac",
                       "spkath", "spkcom", "spkhomo", "spkmil", "spkmslm", "spkrac")

# let's do some additional cleaning as well
df = read.csv("~/repos/signal/gss/gss.csv")

df$colath = df$colath - 4
df$colcom = 5 - df$colcom
df$colhomo = df$colhomo - 4
df$colmil = df$colmil - 4
df$colmslm = df$colmslm - 4
df$colrac = df$colrac - 4

df$libath = 2 - df$libath
df$libcom = 2 - df$libcom
df$libhomo = 2 - df$libhomo
df$libmil = 2 - df$libmil
df$libmslm = 2 - df$libmslm
df$librac = 2 - df$librac

df$spkath = df$spkath - 1
df$spkcom = df$spkcom - 1
df$spkhomo = df$spkhomo - 1
df$spkmil = df$spkmil - 1
df$spkmslm = df$spkmslm - 1
df$spkrac = df$spkrac - 1

# Re-impute and select only the civil liberties questions
fit = softImpute(as.matrix(df), rank.max=16, lambda = 10, type="svd") # maybe I shouldn't re-impute?
ndf = as.data.frame(complete(as.matrix(df), fit))
cl_df = select(ndf, one_of(civil_liberties_qs))
keys = keys[keys$X %in% names(cl_df),]
keys = keys[order(keys$X),]

# re-compute correlation matrix, conduct factor analysis
cm = scree(cl_df)
eig_cor_matrix = eigen(cm)
qplot(1:10, eig_cor_matrix$values[order(abs(eig_cor_matrix$values), decreasing=TRUE)][1:10], ylab = "Eigenvalue") # maybe 9?
scree_n = 1 # try 1-7
# 6 is promising, look at how PA4 stands out

factors = fa(cm, 1, covar=TRUE) # side-effect: corrplot(factors$score.cor)
#corrplot(factors$score.cor) # shows it is 1 factor

# the rotation matrix; needed after imputation
loadings = as.data.frame(factors$loadings[,1:scree_n])

# Grab the related questions for the factors
qs = getQuestions(10, loadings, keys)
qs[,1] # doesn't look good

# # compute the scores (change of basis)
scores = as.matrix(cl_df) %*% as.matrix(factors$loadings[,1:scree_n])

# aggregate by different categories?
factors$loadings
ndf = ndf %>% select(relig, sex, sexornt, reliten, class, wrkgovt, partyid)
ndf = cbind(ndf, scores)

aggregate(ndf, by=ndf["relig"], FUN=mean)
aggregate(ndf, by=ndf["sex"], FUN=mean)
aggregate(ndf, by=ndf["reliten"], FUN=mean)
aggregate(ndf, by=ndf["class"], FUN=mean)
aggregate(ndf, by=ndf["wrkgovt"], FUN=mean)
aggregate(ndf, by=ndf["partyid"], FUN=mean)
# qplot(ndf[managers,"scores"])
# qplot(ndf[engineers,"scores"])
# qplot(ndf[scientists,"scores"])
# qplot(ndf[social_scientists,"scores"])
# qplot(ndf[lawyers,"scores"])
# qplot(ndf[legal,"scores"])
# qplot(ndf[doctors,"scores"])
# qplot(ndf[medical,"scores"])
# qplot(ndf[truckers,"scores"])
# qplot(ndf[military,"scores"])
# qplot(ndf[teachers,"scores"])
# qplot(ndf[under_thirty,"scores"])
# qplot(ndf[thirty_to_sixty,"scores"])
# qplot(ndf[sixty_plus,"scores"])
# 
# mean(ndf[managers,"scores"])
# mean(ndf[engineers,"scores"])
# mean(ndf[scientists,"scores"])
# mean(ndf[social_scientists,"scores"])
# mean(ndf[lawyers,"scores"], na.rm=TRUE)
# mean(ndf[legal,"scores"])
# mean(ndf[doctors,"scores"], na.rm=TRUE)
# mean(ndf[medical,"scores"])
# mean(ndf[truckers,"scores"], na.rm=TRUE)
# mean(ndf[military,"scores"])
# mean(ndf[teachers,"scores"])
# mean(ndf[under_thirty,"scores"], na.rm=TRUE)
# mean(ndf[thirty_to_sixty,"scores"], na.rm=TRUE)
# mean(ndf[sixty_plus,"scores"], na.rm=TRUE)

```

```{r}
qplot(ndf$scores)
# what's with that huge spike at zero?
# zeroes = df[ndf$scores == 0,]
# freespch = as.numeric(ndf$scores == 0)
# df_fs = cbind(df, freespch)
# write.csv(df_fs, "~/repos/signal/gss/gss_fs.csv")


# dim(zeroes)
# sum(is.na(zeroes)) / (318 * 277)
# colSums(is.na(zeroes))
# 
# table(zeroes[,"relig"])
# table(zeroes[,"sex"])
# table(zeroes[,"sexornt"])
# table(zeroes[,"reliten"])
# table(zeroes[,"class"])
# table(zeroes[,"wrkgovt"])
# table(zeroes[,"partyid"])
# table(zeroes[,"age"])
# 
# qplot(relig, reliten, data = remove_missing(select(zeroes, relig, reliten)))
#p = ggplot(data=remove_missing(select(zeroes, relig, reliten)))
#p + geom_point(x = "relig", y = "reliten")
```

